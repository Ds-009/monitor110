{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b43d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV metrics: {'auc_mean': 0.6002099310148373, 'precision_mean': 0.434934218217393, 'recall_mean': 0.054290771236648064, 'f1_mean': 0.09599247352089182}\n",
      "Top risky stocks saved to artifacts\\current_risk_ranking.csv\n",
      "Sample top-10 risky:\n",
      "                        ticker       date  close  prob_plummet  alarm  news_count  avg_sentiment\n",
      "   Hewlett Packard Enterprise 2025-06-30  20.45      0.209981      0         0.0            0.0\n",
      "                       Lowe's 2025-06-30 221.87      0.171666      0         0.0            0.0\n",
      "           UnitedHealth Group 2025-06-30 311.97      0.170386      0         0.0            0.0\n",
      "Huntington Ingalls Industries 2025-06-30 241.46      0.166448      0         0.0            0.0\n",
      "              AES Corporation 2025-06-30  10.52      0.161965      0         0.0            0.0\n",
      "             Procter & Gamble 2025-06-30 159.32      0.157014      0         0.0            0.0\n",
      "              RTX Corporation 2025-06-30 146.02      0.156881      0         0.0            0.0\n",
      "         Ameriprise Financial 2025-06-30 533.73      0.155670      0         0.0            0.0\n",
      "     United Airlines Holdings 2025-06-30  79.63      0.143163      0         0.0            0.0\n",
      "          Insulet Corporation 2025-06-30 314.18      0.143125      0         0.0            0.0\n"
     ]
    }
   ],
   "source": [
    "# Monitor110_v3.0_Precision.py\n",
    "# Changes: Enhanced Sentiment, OBV Indicator, Regularization, Interaction Features\n",
    "\n",
    "import pandas as pd, numpy as np, re\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "import lightgbm as lgb\n",
    "import joblib, os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "PRICE_FN = \"dataset for prediction.csv\"\n",
    "NEWS_FN  = \"financial_news_events.csv\"\n",
    "OUTDIR = \"artifacts_v3\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "# --- 1. ADVANCED FEATURE ENGINEERING ---\n",
    "def calculate_rsi(series, window=14):\n",
    "    delta = series.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def calculate_obv(df):\n",
    "    # On-Balance Volume: Follows the \"smart money\" flow\n",
    "    # If price up, add volume. If price down, subtract volume.\n",
    "    direction = np.where(df['close'] > df['close'].shift(1), 1, -1)\n",
    "    direction[0] = 0\n",
    "    obv = (direction * df['volume']).cumsum()\n",
    "    return obv\n",
    "\n",
    "def add_technical_indicators(df):\n",
    "    df = df.sort_values(['ticker', 'date'])\n",
    "    \n",
    "    # A. RSI (Momentum)\n",
    "    df['rsi_14'] = df.groupby('ticker')['close'].transform(lambda x: calculate_rsi(x, 14)).fillna(50)\n",
    "    \n",
    "    # B. Bollinger Bands (Volatility)\n",
    "    df['ma_20'] = df.groupby('ticker')['close'].transform(lambda x: x.rolling(20).mean())\n",
    "    df['std_20'] = df.groupby('ticker')['close'].transform(lambda x: x.rolling(20).std())\n",
    "    df['bb_upper'] = df['ma_20'] + (df['std_20'] * 2)\n",
    "    df['bb_lower'] = df['ma_20'] - (df['std_20'] * 2)\n",
    "    # Normalized distance: >1 means above bands (Overbought), <0 means below (Oversold)\n",
    "    df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])\n",
    "    \n",
    "    # C. OBV (Volume Trend) - NEW CHANGE\n",
    "    # We apply this per ticker\n",
    "    df['obv'] = df.groupby('ticker', group_keys=False).apply(calculate_obv)\n",
    "    \n",
    "    # D. Moving Average Divergence (Trend) - NEW CHANGE\n",
    "    # How far is price from the 50-day average?\n",
    "    df['ma_50'] = df.groupby('ticker')['close'].transform(lambda x: x.rolling(50, min_periods=1).mean())\n",
    "    df['dist_ma50'] = (df['close'] - df['ma_50']) / df['ma_50']\n",
    "\n",
    "    return df.fillna(0)\n",
    "\n",
    "# --- 2. DATA PROCESSING ---\n",
    "def clean_text(s):\n",
    "    if pd.isna(s): return \"\"\n",
    "    return re.sub(r'http\\S+', '', str(s)).strip()\n",
    "\n",
    "print(\"Loading Data...\")\n",
    "price_df = pd.read_csv(PRICE_FN)\n",
    "news_df = pd.read_csv(NEWS_FN)\n",
    "\n",
    "# Standardizing Columns\n",
    "price_ticker_col = next((c for c in price_df.columns if c.lower() in ['ticker','symbol']), price_df.columns[0])\n",
    "pattern = re.compile(r'(\\d{2}-\\d{2}-\\d{4})_(opening|closing|volume)')\n",
    "date_cols = [c for c in price_df.columns if pattern.match(c)]\n",
    "dates = sorted(list({pattern.match(c).group(1) for c in date_cols}), key=lambda x: datetime.strptime(x, \"%d-%m-%Y\"))\n",
    "\n",
    "rows = []\n",
    "for _, r in price_df.iterrows():\n",
    "    ticker = r.get(price_ticker_col)\n",
    "    for d in dates:\n",
    "        clc = f\"{d}_closing\"\n",
    "        volc = f\"{d}_volume\"\n",
    "        if clc in price_df.columns and pd.notna(r[clc]):\n",
    "            rows.append({\n",
    "                'ticker': ticker, \n",
    "                'date': pd.to_datetime(datetime.strptime(d, \"%d-%m-%Y\")), \n",
    "                'close': r.get(clc, np.nan), \n",
    "                'volume': r.get(volc, np.nan)\n",
    "            })\n",
    "long_price = pd.DataFrame(rows).sort_values(['ticker','date']).reset_index(drop=True)\n",
    "\n",
    "# Process News\n",
    "news_df.columns = [c.strip() for c in news_df.columns]\n",
    "text_col = next((c for c in news_df.columns if 'headline' in c.lower()), news_df.columns[0])\n",
    "date_col = next((c for c in news_df.columns if 'date' in c.lower()), None)\n",
    "news_df['date'] = pd.to_datetime(news_df[date_col], errors='coerce')\n",
    "news_df['headline'] = news_df[text_col].apply(clean_text)\n",
    "\n",
    "# --- CHANGE 2: WEIGHTED SENTIMENT DICTIONARY ---\n",
    "# This gives \"Fraud\" much more weight than just \"Loss\"\n",
    "def expert_sentiment(s):\n",
    "    s = str(s).lower()\n",
    "    score = 0\n",
    "    # CAT 1: Extreme Negatives (Crash Drivers) - Weight -3\n",
    "    score -= sum(3 for k in ['fraud', 'investigation', 'subpoena', 'bankruptcy', 'sanctions', 'breach', 'recall'] if k in s)\n",
    "    # CAT 2: Standard Negatives - Weight -1\n",
    "    score -= sum(1 for k in ['drop', 'fall', 'down', 'loss', 'miss', 'bear', 'weak', 'lower', 'cut'] if k in s)\n",
    "    # CAT 3: Extreme Positives - Weight +2\n",
    "    score += sum(2 for k in ['breakthrough', 'record', 'approval', 'merger', 'acquisition', 'beat'] if k in s)\n",
    "    # CAT 4: Standard Positives - Weight +1\n",
    "    score += sum(1 for k in ['gain', 'rise', 'up', 'growth', 'bull', 'strong', 'raise'] if k in s)\n",
    "    return score\n",
    "\n",
    "news_df['sent_score'] = news_df['headline'].apply(expert_sentiment)\n",
    "\n",
    "# Mapping Logic\n",
    "price_tickers = set(long_price['ticker'].astype(str).unique())\n",
    "def map_to_ticker(row):\n",
    "    rc = str(row.get('Related_Company', ''))\n",
    "    for t in price_tickers:\n",
    "        if t.lower() in rc.lower(): return t\n",
    "    return None\n",
    "news_df['mapped_ticker'] = news_df.apply(map_to_ticker, axis=1)\n",
    "news_agg = news_df[news_df['mapped_ticker'].notna()].groupby(['mapped_ticker','date']).agg(\n",
    "    news_count=('headline','count'), \n",
    "    avg_sentiment=('sent_score','mean')\n",
    ").reset_index().rename(columns={'mapped_ticker':'ticker'})\n",
    "\n",
    "# --- 3. MERGE & INTERACTION FEATURES ---\n",
    "print(\"Building Features...\")\n",
    "df = long_price.merge(news_agg, on=['ticker','date'], how='left').fillna(0)\n",
    "df['return_1d'] = df.groupby('ticker')['close'].pct_change().fillna(0)\n",
    "\n",
    "# Add Indicators\n",
    "df = add_technical_indicators(df)\n",
    "\n",
    "# --- CHANGE 3: INTERACTION FEATURES ---\n",
    "# \"High Volume\" + \"Bad Sentiment\" is the strongest crash signal exists.\n",
    "# We create a feature explicitly for this.\n",
    "df['panic_signal'] = df['volume'] * df['avg_sentiment'] * -1  # (Higher value = More Panic)\n",
    "\n",
    "# Labeling (Target: Drop >= 3% in next 3 days)\n",
    "df['future_min_close'] = df.groupby('ticker')['close'].transform(lambda s: s.shift(-1).rolling(3).min().shift(-2))\n",
    "df['plummet_pct'] = (df['close'] - df['future_min_close']) / df['close']\n",
    "df['plummet_label'] = (df['plummet_pct'] >= 0.03).astype(int)\n",
    "\n",
    "# --- 4. ROBUST TRAINING ---\n",
    "features = [\n",
    "    'news_count', 'avg_sentiment', 'rsi_14', 'bb_position', \n",
    "    'obv', 'dist_ma50', 'panic_signal', 'return_1d'\n",
    "]\n",
    "data = df.dropna(subset=['close']).copy()\n",
    "X = data[features]\n",
    "y = data['plummet_label']\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "models = []\n",
    "\n",
    "# --- CHANGE 4: REGULARIZATION (Prevents Overfitting) ---\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.03,      # Slower = More Accurate\n",
    "    'num_leaves': 20,           # Smaller trees = Less Overfitting\n",
    "    'feature_fraction': 0.8,    # Use 80% of features per tree (Force variety)\n",
    "    'lambda_l1': 1.0,           # L1 Regularization (Noise filtering)\n",
    "    'lambda_l2': 1.0,           # L2 Regularization (Prevent extreme weights)\n",
    "    'scale_pos_weight': 2.5,    # Heavily prioritize detecting crashes\n",
    "    'verbosity': -1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "print(f\"Training Model v3.0 on {len(X)} rows...\")\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "    dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "    model = lgb.train(params, dtrain, num_boost_round=600) # More rounds for slower learning rate\n",
    "    \n",
    "    y_prob = model.predict(X_test)\n",
    "    score = roc_auc_score(y_test, y_prob) if y_test.sum() > 0 else 0\n",
    "    print(f\"Fold {fold+1} AUC: {score:.4f}\")\n",
    "    models.append(model)\n",
    "\n",
    "# --- 5. REPORTING ---\n",
    "best_model = models[-1]\n",
    "joblib.dump(best_model, os.path.join(OUTDIR, 'model_v3_precise.joblib'))\n",
    "\n",
    "latest_indices = df.groupby('ticker')['date'].idxmax()\n",
    "latest_data = df.loc[latest_indices].copy()\n",
    "latest_data['prob_plummet'] = best_model.predict(latest_data[features])\n",
    "latest_data['alarm'] = (latest_data['prob_plummet'] > 0.70).astype(int) # Higher threshold for precision\n",
    "\n",
    "report_cols = ['ticker', 'date', 'close', 'prob_plummet', 'alarm', 'avg_sentiment', 'panic_signal']\n",
    "report = latest_data.sort_values('prob_plummet', ascending=False)[report_cols]\n",
    "report.to_csv(os.path.join(OUTDIR, 'risk_report_v3.csv'), index=False)\n",
    "\n",
    "print(\"\\n--- v3.0 OPTIMIZATION COMPLETE ---\")\n",
    "print(\"Top 5 Risky Stocks (v3.0):\")\n",
    "print(report.head(5).to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
